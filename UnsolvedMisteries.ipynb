{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFpitOiPp3r72YemaIaJ+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panuozzo77/italian-unsolved-misteries/blob/main/UnsolvedMisteries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "556Rrj6yqXc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75SF8TmGoa_u",
        "outputId": "2a0698eb-dda8-4742-a34f-a970b41449c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Ricerca: omicidio Vaticano (Vatican Murder)...\n",
            "🔎 Ricerca: guardia svizzera (Vatican Murder)...\n",
            "🔎 Ricerca: mistero Vaticano (Vatican Murder)...\n",
            "🔎 Ricerca: scomparsa Italia (Disappearance)...\n",
            "🔎 Ricerca: persona scomparsa (Disappearance)...\n",
            "🔎 Ricerca: bambini scomparsi (Disappearance)...\n",
            "🔎 Ricerca: omicidio politico Italia (Politics)...\n",
            "🔎 Ricerca: strage politica (Politics)...\n",
            "🔎 Ricerca: attentato politico (Politics)...\n",
            "🔎 Ricerca: serial killer Italia (Serial Killers)...\n",
            "🔎 Ricerca: assassino seriale (Serial Killers)...\n",
            "🔎 Ricerca: strage italiana (Massacre)...\n",
            "🔎 Ricerca: massacro misterioso (Massacre)...\n",
            "🔎 Ricerca: omicidio mafioso (Mafia)...\n",
            "🔎 Ricerca: crimini mafia Italia (Mafia)...\n",
            "🔎 Ricerca: teoria complotto Italia (Conspiracy)...\n",
            "🔎 Ricerca: misteri italiani (Conspiracy)...\n",
            "🔎 Ricerca: enigmi Italia (Misc)...\n",
            "🔎 Ricerca: casi irrisolti (Misc)...\n",
            "🔎 Ricerca: eventi strani Italia (Misc)...\n",
            "\n",
            "✅ Raccolta dati completata! I risultati sono stati salvati in 'risultati_casi_irrisolti.json'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Configurazione API di Google Custom Search\n",
        "\n",
        "URL_SEARCH = \"https://www.googleapis.com/customsearch/v1\"\n",
        "\n",
        "# Categorie di ricerca e parole chiave\n",
        "categorie = {\n",
        "    \"Vatican Murder\": [\"omicidio Vaticano\", \"guardia svizzera\", \"mistero Vaticano\"],\n",
        "    \"Disappearance\": [\"scomparsa Italia\", \"persona scomparsa\", \"bambini scomparsi\"],\n",
        "    \"Politics\": [\"omicidio politico Italia\", \"strage politica\", \"attentato politico\"],\n",
        "    \"Serial Killers\": [\"serial killer Italia\", \"assassino seriale\"],\n",
        "    \"Massacre\": [\"strage italiana\", \"massacro misterioso\"],\n",
        "    \"Mafia\": [\"omicidio mafioso\", \"crimini mafia Italia\"],\n",
        "    \"Conspiracy\": [\"teoria complotto Italia\", \"misteri italiani\"],\n",
        "    \"Misc\": [\"enigmi Italia\", \"casi irrisolti\", \"eventi strani Italia\"]\n",
        "}\n",
        "\n",
        "def cerca_casi(categoria, query, num_results=1):\n",
        "    \"\"\"Esegue la ricerca su Google Custom Search e restituisce una lista di risultati.\"\"\"\n",
        "    params = {\n",
        "        \"key\": API_KEY,\n",
        "        \"cx\": CX,\n",
        "        \"q\": query,\n",
        "        \"num\": num_results\n",
        "    }\n",
        "    response = requests.get(URL_SEARCH, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    risultati = []\n",
        "    if \"items\" in data:\n",
        "        for item in data[\"items\"]:\n",
        "            risultati.append({\n",
        "                \"categoria\": categoria,\n",
        "                \"titolo\": item[\"title\"],\n",
        "                \"link\": item[\"link\"],\n",
        "                \"descrizione\": item.get(\"snippet\", \"N/A\")\n",
        "            })\n",
        "\n",
        "    return risultati\n",
        "\n",
        "# Eseguiamo le ricerche per ogni categoria\n",
        "tutti_i_risultati = []\n",
        "for categoria, parole_chiave in categorie.items():\n",
        "    for parola in parole_chiave:\n",
        "        print(f\"🔎 Ricerca: {parola} ({categoria})...\")\n",
        "        risultati = cerca_casi(categoria, parola)\n",
        "        tutti_i_risultati.extend(risultati)\n",
        "        time.sleep(1)  # Per evitare rate limit dell'API\n",
        "\n",
        "# Salva i risultati in un file JSON\n",
        "with open(\"risultati_casi_irrisolti.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tutti_i_risultati, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"\\n✅ Raccolta dati completata! I risultati sono stati salvati in 'risultati_casi_irrisolti.json'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RpIT0Vlscz6",
        "outputId": "0e975745-13da-43f0-e4c5-782bdb174c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#******************************************************************************#\n",
        "#                           YOUR ATTENTION PLEASE\n",
        "# SET YOUR PREFERRED LLM MODEL AT THE BOTTOM OF THE FILE\n",
        "# INSIDE THE MAIN FUNCTION\n",
        "#******************************************************************************#\n",
        "import asyncio\n",
        "import os\n",
        "\n",
        "# Set the environment variable for the NVIDIA library path to ensure it uses the GPU\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "async def run(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    p = await asyncio.subprocess.create_subprocess_exec(*cmd,\n",
        "                                                       stdout=asyncio.subprocess.PIPE,\n",
        "                                                       stderr=asyncio.subprocess.PIPE)\n",
        "    return p  # Return the process object\n",
        "\n",
        "async def pipe(lines):\n",
        "    async for line in lines:\n",
        "        print(line.decode().strip())\n",
        "\n",
        "async def start_ollama():\n",
        "    process = await run(['ollama', 'serve'])\n",
        "    return process  # Return the process object\n",
        "\n",
        "async def stop_ollama(process):\n",
        "    if process:\n",
        "        print(\">>> Stopping ollama serve\")\n",
        "        process.terminate()  # Terminate the process\n",
        "        await process.wait()  # Wait for the process to finish\n",
        "\n",
        "async def download_model(model_name, model_numparams):\n",
        "    full_model_name = f\"{model_name}:{model_numparams}b\"\n",
        "    print(f\"I'll download the model: {model_name}\")\n",
        "\n",
        "    # Start the model download and capture output\n",
        "    download_process = await asyncio.create_subprocess_exec(\n",
        "        'ollama', 'pull', model_name,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # Read output and errors\n",
        "    await pipe(download_process.stdout)\n",
        "    await pipe(download_process.stderr)\n",
        "\n",
        "    # Wait for the download to finish\n",
        "    await download_process.wait()\n",
        "\n",
        "async def main():\n",
        "    ollama_process = await start_ollama()\n",
        "#******************************************************************************#\n",
        "              # CHANGE WITH THE ONE YOU LIKE DOWN RIGHT HERE\n",
        "#******************************************************************************#\n",
        "    model_name = 'robomotic/gemma-2-2b-neogenesis-ita'\n",
        "    model_numparams = '1.5'\n",
        "\n",
        "    await download_model(model_name, model_numparams)\n",
        "\n",
        "    # After downloading, stop the ollama serve\n",
        "    await stop_ollama(ollama_process)\n",
        "\n",
        "# Run the main function\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p3eewybOsf6G",
        "outputId": "1e15211a-fad6-472e-98e7-b6a633e38932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> starting ollama serve\n",
            "I'll download the model: robomotic/gemma-2-2b-neogenesis-ita\n",
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest\n",
            "pulling 1dc4051ff9c5... 100% ▕████████████████▏ 5.2 GB\n",
            "pulling 109037bec39c... 100% ▕████████████████▏  136 B\n",
            "pulling 235d5106813e... 100% ▕████████████████▏   81 B\n",
            "pulling 22a838ceb7fb... 100% ▕████████████████▏   84 B\n",
            "pulling 2fcd002ddd44... 100% ▕████████████████▏  486 B\n",
            "verifying sha256 digest\n",
            "writing manifest\n",
            "success \u001b[?25h\n",
            ">>> Stopping ollama serve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "import os\n",
        "import subprocess\n",
        "from bs4 import BeautifulSoup\n",
        "import logging\n",
        "\n",
        "# Configurazione del logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "async def analizza_con_ollama(testo):\n",
        "    \"\"\"Interroga Ollama per categorizzare il testo e estrarre informazioni.\"\"\"\n",
        "    try:\n",
        "        data = {\n",
        "            \"model\": \"robomotic/gemma-2-2b-neogenesis-ita\",  # Modello Ollama\n",
        "            \"prompt\": f\"\"\"Sono un bot, non sono un umano e ti sto usando per analizzare delle informazioni automaticamente.\n",
        "                Analizza il seguente testo e determina se si tratta di un caso di:\n",
        "                Vatican Murder, Disappearance, Politics, Serial Killers, Massacre, Mafia, Conspiracy, Misc.\n",
        "\n",
        "                Rispondi con \"Si\" se il testo descrive uno di questi casi, altrimenti con \"No\".\n",
        "                Se la risposta è \"Si\", indica anche il luogo e la data (se disponibili) del caso in questo modo:\n",
        "                Luogo: <luogo>\n",
        "                Data: <YYYY-MM-DD-hh-mm>\n",
        "                Limitati a rispondere solo in questo modo. Qualsiasi ulteriore parola non sarà considerata.\n",
        "\n",
        "                Testo da analizzare:\n",
        "                {testo}\n",
        "            \"\"\"\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Inviando richiesta a Ollama per analizzare il testo...\")\n",
        "        response = requests.post(\"http://127.0.0.1:11434/api/generate\", json=data)\n",
        "        response.raise_for_status()  # Lancia un'eccezione se la richiesta fallisce\n",
        "\n",
        "        risposta_ollama = response.json()\n",
        "        logger.info(f\"Risposta di Ollama: {risposta_ollama}\")\n",
        "\n",
        "        # Estrai la risposta del modello\n",
        "        model_response = risposta_ollama['response']\n",
        "        logger.info(f\"Risposta del modello: {model_response}\")\n",
        "\n",
        "        # Analizza la risposta del modello per estrarre le informazioni\n",
        "        if \"Si\" in model_response:\n",
        "            luogo = estrai_informazione(model_response, \"Luogo:\")\n",
        "            data_caso = estrai_informazione(model_response, \"Data:\")\n",
        "            logger.info(f\"Caso confermato. Luogo: {luogo}, Data: {data_caso}\")\n",
        "            return True, luogo, data_caso\n",
        "        else:\n",
        "            logger.info(\"Nessun caso valido trovato.\")\n",
        "            return False, None, None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Errore nella comunicazione con Ollama: {e}\")\n",
        "        return False, None, None\n",
        "    except json.JSONDecodeError as e:\n",
        "        logger.error(f\"Errore nella decodifica della risposta JSON di Ollama: {e}\")\n",
        "        return False, None, None\n",
        "\n",
        "def estrai_informazione(testo, chiave):\n",
        "    \"\"\"Estrae un'informazione specifica da un testo usando una chiave.\"\"\"\n",
        "    try:\n",
        "        inizio = testo.find(chiave) + len(chiave)\n",
        "        fine = testo.find(\"\\n\", inizio)  # Trova la fine della riga\n",
        "        if fine != -1:\n",
        "            return testo[inizio:fine].strip()\n",
        "        else:\n",
        "            return testo[inizio:].strip()  # Se non c'è una nuova riga, prendi il resto del testo\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Errore durante l'estrazione dell'informazione: {e}\")\n",
        "        return None\n",
        "\n",
        "async def elabora_risultati(risultati):\n",
        "    \"\"\"Elabora i risultati della ricerca, interrogando Ollama per ciascun risultato.\"\"\"\n",
        "    risultati_filtrati = []\n",
        "    for risultato in risultati:\n",
        "        logger.info(f\"Analisi: {risultato['titolo']} ({risultato['categoria']})...\")\n",
        "        testo_da_analizzare = await estrai_testo_da_url(risultato['link'])  # Estrai testo dalla pagina web\n",
        "        if testo_da_analizzare:\n",
        "            caso_confermato, luogo, data_caso = await analizza_con_ollama(testo_da_analizzare)\n",
        "            if caso_confermato:\n",
        "                risultato['luogo'] = luogo\n",
        "                risultato['data'] = data_caso\n",
        "                risultati_filtrati.append(risultato)\n",
        "        time.sleep(1)  # Per evitare rate limit\n",
        "\n",
        "    return risultati_filtrati\n",
        "\n",
        "async def estrai_testo_da_url(url):\n",
        "    \"\"\"Estrae il testo da una pagina web utilizzando Beautiful Soup.\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Estrazione del testo da: {url}\")\n",
        "        response = requests.get(url, timeout=10)  # Timeout di 10 secondi\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Rimuovi script, stili e altri elementi non testuali\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.extract()\n",
        "        testo = soup.get_text()\n",
        "        logger.info(f\"Testo estratto con successo.\")\n",
        "        return testo.strip()\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Errore nel recupero della pagina {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def cerca_casi(categoria, query, num_results=5):\n",
        "    \"\"\"Esegue la ricerca su Google Custom Search e restituisce una lista di risultati.\"\"\"\n",
        "    params = {\n",
        "        \"key\": API_KEY,\n",
        "        \"cx\": CX,\n",
        "        \"q\": query,\n",
        "        \"num\": num_results\n",
        "    }\n",
        "    try:\n",
        "        logger.info(f\"Ricerca su Google Custom Search: {query} ({categoria})...\")\n",
        "        response = requests.get(URL_SEARCH, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        risultati = []\n",
        "        if \"items\" in data:\n",
        "            for item in data[\"items\"]:\n",
        "                risultati.append({\n",
        "                    \"categoria\": categoria,\n",
        "                    \"titolo\": item[\"title\"],\n",
        "                    \"link\": item[\"link\"],\n",
        "                    \"descrizione\": item.get(\"snippet\", \"N/A\")\n",
        "                })\n",
        "        logger.info(f\"Trovati {len(risultati)} risultati per '{query}'.\")\n",
        "        return risultati\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Errore durante la ricerca per '{query}': {e}\")\n",
        "        return []\n",
        "\n",
        "async def main():\n",
        "    # Eseguiamo le ricerche per ogni categoria\n",
        "    tutti_i_risultati = []\n",
        "    for categoria, parole_chiave in categorie.items():\n",
        "        for parola in parole_chiave:\n",
        "            logger.info(f\"Ricerca: {parola} ({categoria})...\")\n",
        "            risultati = cerca_casi(categoria, parola)\n",
        "            tutti_i_risultati.extend(risultati)\n",
        "            time.sleep(1)  # Per evitare rate limit dell'API\n",
        "\n",
        "    # Analizza i risultati con Ollama\n",
        "    risultati_filtrati = await elabora_risultati(tutti_i_risultati)\n",
        "\n",
        "    # Salva i risultati filtrati in un file JSON\n",
        "    with open(\"risultati_filtrati.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(risultati_filtrati, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    logger.info(\"\\n✅ Analisi completata! I risultati filtrati sono stati salvati in 'risultati_filtrati.json'.\")\n",
        "\n",
        "# Esegui la funzione principale\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        logger.info(\"Avvio del servizio Ollama...\")\n",
        "        ollama_process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        time.sleep(5)  # Attendere che Ollama sia pronto\n",
        "        asyncio.run(main())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Errore durante l'esecuzione del programma: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWdooS9LwkgS",
        "outputId": "a698c4a3-3ebc-4150-e0bb-c19a732d2031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Errore durante l'esecuzione del programma: asyncio.run() cannot be called from a running event loop\n",
            "<ipython-input-9-7976dfddeb3d>:170: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  logger.error(f\"Errore durante l'esecuzione del programma: {e}\")\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdcgbbHF68bN",
        "outputId": "8997715b-2b26-4e8f-a840-d12b5e4fe035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "import os\n",
        "import subprocess\n",
        "from bs4 import BeautifulSoup\n",
        "import logging\n",
        "import nest_asyncio # ADD THIS LINE\n",
        "\n",
        "# Configurazione del logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "URL_SEARCH = \"https://www.googleapis.com/customsearch/v1\"\n",
        "\n",
        "categorie = {\n",
        "    \"Vatican Murder\": [\"omicidio vaticano\", \"morte sospetta vaticano\"],\n",
        "    \"Disappearance\": [\"scomparsa misteriosa\", \"persona scomparsa\"],\n",
        "    \"Politics\": [\"scandalo politico\", \"corruzione politica\"],\n",
        "    \"Serial Killers\": [\"serial killer\", \"omicidi seriali\"],\n",
        "    \"Massacre\": [\"massacro\", \"strage\"],\n",
        "    \"Mafia\": [\"mafia\", \"criminalità organizzata\"],\n",
        "    \"Conspiracy\": [\"teoria del complotto\", \"cospirazione\"],\n",
        "    \"Misc\": [\"crimine insolito\", \"evento misterioso\"]\n",
        "}\n",
        "\n",
        "async def analizza_con_ollama(testo):\n",
        "    \"\"\"Interroga Ollama per categorizzare il testo e estrarre informazioni.\"\"\"\n",
        "    try:\n",
        "        data = {\n",
        "            \"model\": \"robomotic/gemma-2-2b-neogenesis-ita\",  # Modello Ollama\n",
        "            \"prompt\": f\"\"\"Sono un bot, non sono un umano e ti sto usando per analizzare delle informazioni automaticamente.\n",
        "                Analizza il seguente testo e determina se si tratta di un caso di:\n",
        "                Vatican Murder, Disappearance, Politics, Serial Killers, Massacre, Mafia, Conspiracy, Misc.\n",
        "\n",
        "                Rispondi con \"Si\" se il testo descrive uno di questi casi, altrimenti con \"No\".\n",
        "                Se la risposta è \"Si\", indica anche il luogo e la data (se disponibili) del caso in questo modo:\n",
        "                Luogo: <luogo>\n",
        "                Data: <YYYY-MM-DD-hh-mm>\n",
        "                Limitati a rispondere solo in questo modo. Qualsiasi ulteriore parola non sarà considerata.\n",
        "\n",
        "                Testo da analizzare:\n",
        "                {testo}\n",
        "            \"\"\"\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Inviando richiesta a Ollama per analizzare il testo...\")\n",
        "        response = requests.post(\"http://127.0.0.1:11434/api/generate\", json=data)\n",
        "        response.raise_for_status()  # Lancia un'eccezione se la richiesta fallisce\n",
        "\n",
        "        risposta_ollama = response.json()\n",
        "        logger.info(f\"Risposta di Ollama: {risposta_ollama}\")\n",
        "\n",
        "        # Estrai la risposta del modello\n",
        "        model_response = risposta_ollama['response']\n",
        "        logger.info(f\"Risposta del modello: {model_response}\")\n",
        "\n",
        "        # Analizza la risposta del modello per estrarre le informazioni\n",
        "        if \"Si\" in model_response:\n",
        "            luogo = estrai_informazione(model_response, \"Luogo:\")\n",
        "            data_caso = estrai_informazione(model_response, \"Data:\")\n",
        "            logger.info(f\"Caso confermato. Luogo: {luogo}, Data: {data_caso}\")\n",
        "            return True, luogo, data_caso\n",
        "        else:\n",
        "            logger.info(\"Nessun caso valido trovato.\")\n",
        "            return False, None, None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Errore nella comunicazione con Ollama: {e}\")\n",
        "        return False, None, None\n",
        "    except json.JSONDecodeError as e:\n",
        "        logger.error(f\"Errore nella decodifica della risposta JSON di Ollama: {e}\")\n",
        "        return False, None, None\n",
        "\n",
        "def estrai_informazione(testo, chiave):\n",
        "    \"\"\"Estrae un'informazione specifica da un testo usando una chiave.\"\"\"\n",
        "    try:\n",
        "        inizio = testo.find(chiave) + len(chiave)\n",
        "        fine = testo.find(\"\\n\", inizio)  # Trova la fine della riga\n",
        "        if fine != -1:\n",
        "            return testo[inizio:fine].strip()\n",
        "        else:\n",
        "            return testo[inizio:].strip()  # Se non c'è una nuova riga, prendi il resto del testo\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Errore durante l'estrazione dell'informazione: {e}\")\n",
        "        return None\n",
        "\n",
        "async def elabora_risultati(risultati):\n",
        "    \"\"\"Elabora i risultati della ricerca, interrogando Ollama per ciascun risultato.\"\"\"\n",
        "    risultati_filtrati = []\n",
        "    for risultato in risultati:\n",
        "        logger.info(f\"Analisi: {risultato['titolo']} ({risultato['categoria']})...\")\n",
        "        testo_da_analizzare = await estrai_testo_da_url(risultato['link'])  # Estrai testo dalla pagina web\n",
        "        if testo_da_analizzare:\n",
        "            caso_confermato, luogo, data_caso = await analizza_con_ollama(testo_da_analizzare)\n",
        "            if caso_confermato:\n",
        "                risultato['luogo'] = luogo\n",
        "                risultato['data'] = data_caso\n",
        "                risultati_filtrati.append(risultato)\n",
        "        time.sleep(1)  # Per evitare rate limit\n",
        "\n",
        "    return risultati_filtrati\n",
        "\n",
        "async def estrai_testo_da_url(url):\n",
        "    \"\"\"Estrae il testo da una pagina web utilizzando Beautiful Soup.\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Estrazione del testo da: {url}\")\n",
        "        response = requests.get(url, timeout=10)  # Timeout di 10 secondi\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Rimuovi script, stili e altri elementi non testuali\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.extract()\n",
        "        testo = soup.get_text()\n",
        "        logger.info(f\"Testo estratto con successo.\")\n",
        "        return testo.strip()\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Errore nel recupero della pagina {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def cerca_casi(categoria, query, num_results=5):\n",
        "    \"\"\"Esegue la ricerca su Google Custom Search e restituisce una lista di risultati.\"\"\"\n",
        "    params = {\n",
        "        \"key\": API_KEY,\n",
        "        \"cx\": CX,\n",
        "        \"q\": query,\n",
        "        \"num\": num_results\n",
        "    }\n",
        "    try:\n",
        "        logger.info(f\"Ricerca su Google Custom Search: {query} ({categoria})...\")\n",
        "        response = requests.get(URL_SEARCH, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        risultati = []\n",
        "        if \"items\" in data:\n",
        "            for item in data[\"items\"]:\n",
        "                risultati.append({\n",
        "                    \"categoria\": categoria,\n",
        "                    \"titolo\": item[\"title\"],\n",
        "                    \"link\": item[\"link\"],\n",
        "                    \"descrizione\": item.get(\"snippet\", \"N/A\")\n",
        "                })\n",
        "        logger.info(f\"Trovati {len(risultati)} risultati per '{query}'.\")\n",
        "        return risultati\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Errore durante la ricerca per '{query}': {e}\")\n",
        "        return []\n",
        "\n",
        "async def main():\n",
        "    # Eseguiamo le ricerche per ogni categoria\n",
        "    tutti_i_risultati = []\n",
        "    for categoria, parole_chiave in categorie.items():\n",
        "        for parola in parole_chiave:\n",
        "            logger.info(f\"Ricerca: {parola} ({categoria})...\")\n",
        "            risultati = cerca_casi(categoria, parola)\n",
        "            tutti_i_risultati.extend(risultati)\n",
        "            time.sleep(1)  # Per evitare rate limit dell'API\n",
        "\n",
        "    # Analizza i risultati con Ollama\n",
        "    risultati_filtrati = await elabora_risultati(tutti_i_risultati)\n",
        "\n",
        "    # Salva i risultati filtrati in un file JSON\n",
        "    with open(\"risultati_filtrati.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(risultati_filtrati, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    logger.info(\"\\n✅ Analisi completata! I risultati filtrati sono stati salvati in 'risultati_filtrati.json'.\")\n",
        "\n",
        "# Esegui la funzione principale\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        logger.info(\"Avvio del servizio Ollama...\")\n",
        "        ollama_process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        time.sleep(5)  # Attendere che Ollama sia pronto\n",
        "        nest_asyncio.apply() # ADD THIS LINE HERE!\n",
        "        asyncio.run(main())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Errore durante l'esecuzione del programma: {e}\")"
      ],
      "metadata": {
        "id": "xf-Zc_s2zA6r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}